{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:01:01.550091Z",
     "start_time": "2020-01-07T16:01:01.538094Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.utils.data\n",
    "from torch.nn import DataParallel\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from backbone.cbam import CBAMResNet, CBAMResNet_ae\n",
    "from margin.ArcMarginProduct import ArcMarginProduct\n",
    "from margin.MultiMarginProduct import MultiMarginProduct\n",
    "from margin.CosineMarginProduct import CosineMarginProduct\n",
    "from margin.InnerProduct import InnerProduct\n",
    "from utils.logging import init_log\n",
    "from dataset.casia_webface import CASIAWebFace\n",
    "from dataset.lfw import LFW\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "from eval_lfw import evaluation_10_fold, getFeatureFromTorch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:01:02.441253Z",
     "start_time": "2020-01-07T16:01:02.216574Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.ToTensor()])  \n",
    "\n",
    "unloader = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:01:03.307818Z",
     "start_time": "2020-01-07T16:01:03.298584Z"
    }
   },
   "outputs": [],
   "source": [
    "train_root = '../CASIA-WebFace' \n",
    "train_file_list = 'names_2000.txt'\n",
    "\n",
    "lfw_test_root = '../Siamese_lfw_pytorch-master/lfw' \n",
    "lfw_file_list = './pairs.txt'\n",
    "\n",
    "backbone =  'Res50_IR' \n",
    "margin_type = 'Softmax'\n",
    "feature_dim = 512   \n",
    "scale_size =  32  \n",
    "batch_size =  64   \n",
    "total_epoch = 40   \n",
    "\n",
    "save_freq =  300  \n",
    "test_freq =  300  \n",
    "resume =  False  \n",
    "net_path =  '' \n",
    "margin_path =  ''   \n",
    "save_dir =  './model'  \n",
    "model_pre =  '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:01:04.505852Z",
     "start_time": "2020-01-07T16:01:04.466351Z"
    }
   },
   "outputs": [],
   "source": [
    "class printer(nn.Module):\n",
    "    def forward(self, input):\n",
    "        print(input.size())\n",
    "        return input\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=256):\n",
    "        return input.view(input.size(0), size, 5, 5)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=128):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            #printer(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            #printer(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            #printer(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            #printer(),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        ).to(device)\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim).to(device)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim).to(device)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim).to(device)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
    "            #printer(),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
    "            #printer(),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, output_padding = 1),\n",
    "            #printer(),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=4, stride=2),\n",
    "            #printer(),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size()).to(device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    def representation(self, x):\n",
    "        return self.bottleneck(self.encoder(x))[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x) # b 6400\n",
    "        #print(h.shape)\n",
    "        z, mu, logvar = self.bottleneck(h)#b 32, b 32, b 32\n",
    "        z = self.fc3(z)#b 6400\n",
    "        return self.decoder(h), mu, logvar, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:01:08.287961Z",
     "start_time": "2020-01-07T16:01:05.111895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size:  96968 / 2000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "save_dir = os.path.join(save_dir, model_pre + backbone.upper() + '_' + datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "if os.path.exists(save_dir):\n",
    "    raise NameError('model dir exists!')\n",
    "os.makedirs(save_dir)\n",
    "logging = init_log(save_dir)\n",
    "_print = logging.info\n",
    "\n",
    "# dataset loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor()  # range [0.0, 1. -> [-1.0,1.0]\n",
    "])\n",
    "# validation dataset\n",
    "trainset = CASIAWebFace(train_root, train_file_list, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512,\n",
    "                                        shuffle=True, num_workers=4, drop_last=False)\n",
    "\n",
    "\n",
    "vae = VAE(image_channels=3, h_dim=6400, z_dim=32).to(device)\n",
    "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:01:08.294355Z",
     "start_time": "2020-01-07T16:01:08.290599Z"
    }
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer):\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:01:08.902359Z",
     "start_time": "2020-01-07T16:01:08.894790Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "\n",
    "    BCE = F.mse_loss(recon_x, x, size_average=False).to(device)\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()).to(device)\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:01:09.795451Z",
     "start_time": "2020-01-07T16:01:09.788120Z"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)  # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:01:12.192368Z",
     "start_time": "2020-01-07T16:01:12.188521Z"
    }
   },
   "outputs": [],
   "source": [
    "vae_file_name = 'vae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T16:51:27.379887Z",
     "start_time": "2019-12-22T16:51:27.373898Z"
    }
   },
   "outputs": [],
   "source": [
    "# epochs_ae = 50\n",
    "# max_mse = 9999\n",
    "\n",
    "# for epoch in range(epochs_ae):\n",
    "#     vae.train()\n",
    "#     loss_epoch = []\n",
    "#     if (epoch+1)%10 == 0:\n",
    "#         adjust_learning_rate(vae_optimizer)\n",
    "        \n",
    "#     for idx, (images, _) in enumerate(trainloader):\n",
    "        \n",
    "#         recon_images, mu, logvar, h = vae(images.to(device))\n",
    "#         #print(images[0])\n",
    "        \n",
    "        \n",
    "#         #print(recon_images)\n",
    "#         loss, bce, kld = loss_fn(recon_images, images.to(device), mu, logvar)\n",
    "#         vae_optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         vae_optimizer.step()\n",
    "#         loss_epoch.append(bce.cpu().detach().numpy()/512)\n",
    "#         if (idx)%100 == 0:\n",
    "#             imshow(images[0].cpu().detach(), title=None)\n",
    "#             imshow(recon_images[0].cpu().detach(), title=None)\n",
    "#             to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.6f}\".format(epoch+1, \n",
    "#                                 epochs_ae, loss.item()/512, bce.item()/512, kld.item()/512)\n",
    "#             print(to_print)\n",
    "        \n",
    "            \n",
    "            \n",
    "#     cur_mse = np.mean(np.array(loss_epoch))\n",
    "#     print(\"Epoch[{}/{}] Loss: {:.6f}\".format(epoch+1, \n",
    "#                                 epochs_ae, cur_mse))\n",
    "#     if cur_mse < max_mse:\n",
    "#             max_mse = cur_mse\n",
    "#             state = {\n",
    "#                 'net': vae.state_dict(),\n",
    "#                 'optimizer': vae_optimizer.state_dict(),\n",
    "#                 'epoch': epoch\n",
    "#             }\n",
    "#             torch.save(state, vae_file_name)\n",
    "#             print('\\n------------ Save best model ------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:01:13.941116Z",
     "start_time": "2020-01-07T16:01:13.902168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (7): ReLU()\n",
       "    (8): Flatten()\n",
       "  )\n",
       "  (fc1): Linear(in_features=6400, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=6400, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=6400, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): UnFlatten()\n",
       "    (1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), output_padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(vae_file_name)\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "vae.load_state_dict(checkpoint['net'])\n",
    "vae_optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "vae = vae.to(device)\n",
    "vae.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "#device = torch.device('cpu')\n",
    "# log init\n",
    "save_dir = os.path.join(save_dir, model_pre + backbone.upper() + '_' + datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "if os.path.exists(save_dir):\n",
    "    raise NameError('model dir exists!')\n",
    "os.makedirs(save_dir)\n",
    "logging = init_log(save_dir)\n",
    "_print = logging.info\n",
    "\n",
    "# dataset loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# validation dataset\n",
    "trainset = CASIAWebFace(train_root, train_file_list, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=96,\n",
    "                                          shuffle=True, num_workers=4, drop_last=False)\n",
    "# test dataset\n",
    "lfwdataset = LFW(lfw_test_root, lfw_file_list, transform=transform)\n",
    "lfwloader = torch.utils.data.DataLoader(lfwdataset, batch_size=96,\n",
    "                                        shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "\n",
    "\n",
    "if backbone == 'MobileFace':\n",
    "    net = MobileFaceNet()\n",
    "elif backbone == 'Res50_IR':\n",
    "    net = CBAMResNet(50, feature_dim=feature_dim, mode='ir')\n",
    "elif backbone == 'SERes50_IR':\n",
    "    net = CBAMResNet(50, feature_dim=args.feature_dim, mode='ir_se')\n",
    "elif backbone == 'Res100_IR':\n",
    "    net = CBAMResNet(100, feature_dim=args.feature_dim, mode='ir')\n",
    "elif backbone == 'SERes100_IR':\n",
    "    net = CBAMResNet(100, feature_dim=args.feature_dim, mode='ir_se')\n",
    "else:\n",
    "    print(backbone, ' is not available!')\n",
    "\n",
    "net = CBAMResNet_ae(device, vae, 50, feature_dim=feature_dim, mode='ir').to(device)\n",
    "\n",
    "if margin_type == 'ArcFace':\n",
    "    margin = ArcMarginProduct(feature_dim, trainset.class_nums, s=scale_size)\n",
    "elif margin_type == 'MultiMargin':\n",
    "    margin = MultiMarginProduct(args.feature_dim, trainset.class_nums, s=args.scale_size)\n",
    "elif margin_type == 'CosFace':\n",
    "    margin = CosineMarginProduct(args.feature_dim, trainset.class_nums, s=args.scale_size)\n",
    "elif margin_type == 'Softmax':\n",
    "    margin = InnerProduct(feature_dim, trainset.class_nums)\n",
    "else:\n",
    "    print(margin_type, 'is not available!')\n",
    "\n",
    "if resume:\n",
    "    print('resume the model parameters from: ', net_path, margin_path)\n",
    "    net.load_state_dict(torch.load(net_path)['net_state_dict'])\n",
    "    margin.load_state_dict(torch.load(margin_path)['net_state_dict'])\n",
    "\n",
    "# define optimizers for different layer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer_ft = optim.SGD([\n",
    "    {'params': net.parameters(), 'weight_decay': 5e-4},\n",
    "    {'params': margin.parameters(), 'weight_decay': 5e-4}\n",
    "], lr=0.1, momentum=0.9, nesterov=True)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.MultiStepLR(optimizer_ft, milestones=[8, 13, 18, 23, 28, 33, 38], gamma=0.2)\n",
    "\n",
    "net = net.to(device)\n",
    "margin = margin.to(device)\n",
    "\n",
    "\n",
    "best_lfw_acc = 0.0\n",
    "best_lfw_iters = 0\n",
    "\n",
    "total_iters = 0\n",
    "\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "    exp_lr_scheduler.step()\n",
    "    # train model\n",
    "    print('Train Epoch: {}/{} ...'.format(epoch, total_epoch))\n",
    "    net.train()\n",
    "\n",
    "    since = time.time()\n",
    "    for data in trainloader:\n",
    "        img, label = data[0].to(device), data[1].to(device)\n",
    "        #print(img.shape)\n",
    "        optimizer_ft.zero_grad()\n",
    "\n",
    "        raw_logits = net(img)\n",
    "        output = margin(raw_logits, label)\n",
    "        total_loss = criterion(output, label)\n",
    "        total_loss.backward()\n",
    "        optimizer_ft.step()\n",
    "\n",
    "        total_iters += 1\n",
    "        # print train information\n",
    "        if total_iters % 100 == 0:\n",
    "            \n",
    "            # current training accuracy\n",
    "            _, predict = torch.max(output.data, 1)\n",
    "            total = label.size(0)\n",
    "            correct = (np.array(predict.cpu()) == np.array(label.data.cpu())).sum()\n",
    "            time_cur = (time.time() - since) / 100\n",
    "            since = time.time()\n",
    "\n",
    "\n",
    "            print(\"Iters: {:0>6d}/[{:0>2d}], loss: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}\".format(total_iters, epoch, total_loss.item(), correct/total, time_cur, exp_lr_scheduler.get_lr()[0]))\n",
    "\n",
    "        # save model\n",
    "        if total_iters % save_freq == 0:\n",
    "            msg = 'Saving checkpoint: {}'.format(total_iters)\n",
    "            print(msg)\n",
    "            \n",
    "            net_state_dict = net.state_dict()\n",
    "            margin_state_dict = margin.state_dict()\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.mkdir(save_dir)\n",
    "            torch.save({\n",
    "                'iters': total_iters,\n",
    "                'net_state_dict': net_state_dict},\n",
    "                os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))\n",
    "            torch.save({\n",
    "                'iters': total_iters,\n",
    "                'net_state_dict': margin_state_dict},\n",
    "                os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))\n",
    "\n",
    "        # test accuracy\n",
    "        if total_iters % test_freq == 0:\n",
    "            with torch.no_grad(): \n",
    "\n",
    "                # test model on lfw\n",
    "                net.eval()\n",
    "                getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)\n",
    "                lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')\n",
    "                print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))\n",
    "                if best_lfw_acc <= np.mean(lfw_accs) * 100:\n",
    "                    best_lfw_acc = np.mean(lfw_accs) * 100\n",
    "                    best_lfw_iters = total_iters\n",
    "\n",
    "\n",
    "        net.train()\n",
    "\n",
    "print('Finally Best Accuracy: LFW: {:.4f} in iters: {}'.format(best_lfw_acc, best_lfw_iters))\n",
    "print('finishing training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T13:49:26.481355Z",
     "start_time": "2019-12-21T13:49:26.362Z"
    }
   },
   "outputs": [],
   "source": [
    "%run dataset/casia_webface.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base1)",
   "language": "python",
   "name": "base1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}